{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408f38cd",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Meta/blob/master/Demo_China_A_share_market.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f3a7e",
   "metadata": {
    "id": "ef0f3a7e"
   },
   "source": [
    "# Demo: ElegantRL_HelloWorld_tutorial (DQN, DDPG, PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890eac1c",
   "metadata": {},
   "source": [
    "![File_structure of ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/figs/File_structure.png)\n",
    "\n",
    "One sentence summary: an agent (agent.py) with Actor-Critic networks (net.py) is trained (run.py) by interacting with an environment (env.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a0836",
   "metadata": {},
   "source": [
    "### The training env for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Q8gKimq2PZDh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q8gKimq2PZDh",
    "outputId": "ea18fe12-1b5e-492e-fa4a-53f5fa776694"
   },
   "outputs": [],
   "source": [
    "from elegantrl_helloworld.config import Arguments\n",
    "from elegantrl_helloworld.run import train_agent, evaluate_agent",
    "from elegantrl_helloworld.env import get_gym_env_args, PendulumEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb79afd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDejJbjYQuUi",
    "outputId": "13f9a389-1113-4a67-b962-68a29ece6c21",
    "scrolled": true
   },
   "source": [
    "Install `gym` to run some `env` for DRL training.\n",
    "- [Link to know more about **Discreted action** space env `CartPole`](https://gym.openai.com/envs/CartPole-v1/)\n",
    "- [Link to know more about **Discreted action** space env `LunarLander`](https://gym.openai.com/envs/LunarLander-v2/)\n",
    "- [Link to know more about **Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/)\n",
    "- [Link to know more about **Continuous action** space env `LunarLanderContinuous`](https://gym.openai.com/envs/LunarLanderContinuous-v2/)\n",
    "- [Link to know more about **Continuous action** space env `BipedalWalker`](https://gym.openai.com/envs/BipedalWalker-v2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bada3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from gym) (1.21.5)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from gym) (4.11.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed014d",
   "metadata": {},
   "source": [
    "Install `Box2D` to run some `env` for DRL training.\n",
    "\n",
    "Box2D is a 2D rigid body simulation library for games.\n",
    "\n",
    "The following code install `Box2D` for task `LunarLannder` and `BipdealWalker`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3729f70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Box2D\n",
      "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Box2D\n",
      "Successfully installed Box2D-2.3.10\n",
      "Requirement already satisfied: box2d-py in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (2.3.8)\n",
      "Requirement already satisfied: gym[Box_2D] in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (0.21.0)\n",
      "\u001b[33mWARNING: gym 0.21.0 does not provide the extra 'box_2d'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from gym[Box_2D]) (1.21.5)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from gym[Box_2D]) (4.11.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from gym[Box_2D]) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym[Box_2D]) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /xfs/home/podracer_steven/anaconda3/envs/mujoco/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym[Box_2D]) (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install Box2D\n",
    "!pip3 install box2d-py\n",
    "!pip3 install gym[Box_2D]\n",
    "\n",
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29be50",
   "metadata": {},
   "source": [
    "## Train DQN on discreted action space task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec14db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_args = {'env_num': 1,\n",
      "            'env_name': 'CartPole-v0',\n",
      "            'max_step': 200,\n",
      "            'state_dim': 4,\n",
      "            'action_dim': 2,\n",
      "            'if_discrete': True}\n"
     ]
    }
   ],
   "source": [
    "from elegantrl_helloworld.agent import AgentDQN\n",
    "agent_class = AgentDQN\n",
    "env_name = [\"CartPole-v0\", \"LunarLander-v2\"][0]\n",
    "\n",
    "if env_name == \"CartPole-v0\":\n",
    "    import gym\n",
    "    env = gym.make(env_name)\n",
    "    env_func = gym.make\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "    args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "    '''reward shaping'''\n",
    "    args.reward_scale = 2 ** 0\n",
    "    args.gamma = 0.97\n",
    "\n",
    "    '''network update'''\n",
    "    args.target_step = args.max_step * 2\n",
    "    args.net_dim = 2 ** 7\n",
    "    args.batch_size = 2 ** 7\n",
    "    args.repeat_times = 2 ** 0\n",
    "    args.explore_rate = 0.25\n",
    "\n",
    "    '''evaluate'''\n",
    "    args.eval_gap = 2 ** 5\n",
    "    args.eval_times = 2 ** 3\n",
    "    args.break_step = int(1e5)\n",
    "elif env_name == \"LunarLander-v2\":\n",
    "    import gym\n",
    "    env = gym.make(env_name)\n",
    "    env_func = gym.make\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "    args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "    '''reward shaping'''\n",
    "    args.reward_scale = 2 ** 0\n",
    "    args.gamma = 0.99\n",
    "\n",
    "    '''network update'''\n",
    "    args.target_step = args.max_step\n",
    "    args.net_dim = 2 ** 7\n",
    "    args.batch_size = 2 ** 7\n",
    "    args.repeat_times = 2 ** 0\n",
    "    args.explore_noise = 0.125\n",
    "\n",
    "    '''evaluate'''\n",
    "    args.eval_gap = 2 ** 7\n",
    "    args.eval_times = 2 ** 4\n",
    "    args.break_step = int(4e5)\n",
    "else:\n",
    "    raise ValueError(\"env_name:\", env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978812c",
   "metadata": {},
   "source": [
    "`env_args = get_gym_env_args(env, if_print=True)` print the information about the `env`\n",
    "\n",
    "Then, choose the GPU. (set as `-1` or GPU unavaliable, it will use CPU automatically.)\n",
    "\n",
    "Finally, train and evaluate the DRL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de98a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
        "| Arguments Remove cwd: ./CartPole-v0_DQN_6 \n",
        "| Step 4.51e+02  ExpR     1.00  | ObjC     0.26  ObjA     0.29 \n",
        "| Step 4.14e+04  ExpR     1.00  | ObjC     0.25  ObjA    31.08 \n",
        "| Step 6.81e+04  ExpR     1.00  | ObjC     0.71  ObjA    33.63 \n",
        "| Step 8.92e+04  ExpR     1.00  | ObjC     0.39  ObjA    33.55 \n",
        "| UsedTime: 234 | SavedDir: ./CartPole-v0_DQN_6 \n",
        "\n",
        "| Arguments Keep cwd: ./CartPole-v0_DQN_6 \n",
        "| Steps 4.51e+02  | Returns avg    89.125  std    13.364 \n",
        "| Steps 4.14e+04  | Returns avg   194.625  std    11.124 \n",
        "| Steps 6.81e+04  | Returns avg   199.125  std     2.315 \n",
        "| Steps 8.92e+04  | Returns avg   200.000  std     0.000 \n"
     ]
    }
   ],
    "source": [
    "!conda activate \n",
    "args.learner_gpus = 0\n",
    "\n",
    "train_agent(args)\n",
    "evaluate_agent(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54077e6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0-reEAYJTkU",
    "outputId": "2d6895dc-93a4-4661-934d-8bf1cd857e9c"
   },
   "source": [
    "## Train DDPG on continuous action space task.\n",
    "\n",
    "The following code just for tutorial.\n",
    "\n",
    "DDPG is a simple DRL algorithm. But it is low sample efficiency and unstable.\n",
    "\n",
    "Remember to run the PPO below later to experience how the **PPO algorithm is better than the DDPG algorithm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1cd1fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
        "| Arguments Remove cwd: ./Pendulum-v1_DDPG_6 \n",
        "|Step 4.00e+02  ExpR    -3.60  |ObjC     3.22  ObjA     0.15 \n",
        "|Step 4.00e+04  ExpR    -1.98  |ObjC     0.86  ObjA   -82.97 \n",
        "|Step 5.88e+04  ExpR    -1.58  |ObjC     0.82  ObjA   -66.20 \n",
        "|Step 7.32e+04  ExpR    -0.60  |ObjC     0.62  ObjA   -45.66 \n",
        "|Step 8.52e+04  ExpR    -0.47  |ObjC     0.36  ObjA   -33.60 \n",
        "|Step 9.56e+04  ExpR    -0.33  |ObjC     0.35  ObjA   -28.88 \n",
        "| UsedTime: 357 | SavedDir: ./Pendulum-v1_DDPG_6 \n",
        "\n",
        "| Arguments Keep cwd: ./Pendulum-v1_DDPG_6 \n",
        "|Steps          400  |Returns avg -1391.019  std   272.423 \n",
        "|Steps        40000  |Returns avg  -822.530  std    77.746 \n",
        "|Steps        58800  |Returns avg  -583.974  std    54.622 \n",
        "|Steps        73200  |Returns avg  -199.278  std    83.178 \n",
        "|Steps        85200  |Returns avg  -163.388  std    82.727 \n",
        "|Steps        95600  |Returns avg  -211.675  std    72.861 \n"
     ]
    }
   ],
   "source": [
    "from elegantrl_helloworld.agent import AgentDDPG\n",
    "agent_class = AgentDDPG\n",
    "env_name = [\"Pendulum-v1\", \"LunarLanderContinuous-v2\", \"BipedalWalker-v3\"][0]\n",
    "gpu_id = 0\n",
    "\n",
    "if env_name == \"Pendulum-v1\":\n",
    "    env = PendulumEnv()\n",
    "    env_func = PendulumEnv\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "    args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "    '''reward shaping'''\n",
    "    args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
    "    args.gamma = 0.97\n",
    "\n",
    "    '''network update'''\n",
    "    args.target_step = args.max_step * 2\n",
    "    args.net_dim = 2 ** 7\n",
    "    args.batch_size = 2 ** 7\n",
    "    args.repeat_times = 2 ** 0\n",
    "    args.explore_noise = 0.1\n",
    "\n",
    "    '''evaluate'''\n",
    "    args.eval_gap = 2 ** 6\n",
    "    args.eval_times = 2 ** 3\n",
    "    args.break_step = int(1e5)\n",
    "elif env_name == \"LunarLanderContinuous-v2\":\n",
    "    import gym\n",
    "    env = gym.make(env_name)\n",
    "    env_func = gym.make\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "    args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "    '''reward shaping'''\n",
    "    args.reward_scale = 2 ** 0\n",
    "    args.gamma = 0.99\n",
    "\n",
    "    '''network update'''\n",
    "    args.target_step = args.max_step // 2\n",
    "    args.net_dim = 2 ** 7\n",
    "    args.batch_size = 2 ** 7\n",
    "    args.repeat_times = 2 ** 0\n",
    "    args.explore_noise = 0.1\n",
    "\n",
    "    '''evaluate'''\n",
    "    args.eval_gap = 2 ** 7\n",
    "    args.eval_times = 2 ** 4\n",
    "    args.break_step = int(4e5)\n",
    "elif env_name == \"BipedalWalker-v3\":\n",
    "    import gym\n",
    "    env = gym.make(env_name)\n",
    "    env_func = gym.make\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "    args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "    '''reward shaping'''\n",
    "    args.reward_scale = 2 ** -1\n",
    "    args.gamma = 0.98\n",
    "\n",
    "    '''network update'''\n",
    "    args.target_step = args.max_step // 2\n",
    "    args.net_dim = 2 ** 8\n",
    "    args.batch_size = 2 ** 8\n",
    "    args.repeat_times = 2 ** 0\n",
    "    args.explore_noise = 0.05\n",
    "\n",
    "    '''evaluate'''\n",
    "    args.eval_gap = 2 ** 7\n",
    "    args.eval_times = 2 ** 3\n",
    "    args.break_step = int(3e5)\n",
    "else:\n",
    "    raise ValueError(\"env_name:\", env_name)\n",
    "\n",
    "args.learner_gpus = gpu_id\n",
    "train_agent(args)\n",
    "evaluate_agent(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b4c2a",
   "metadata": {},
   "source": [
    "## ## Train PPO on continuous action space task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fluid-taylor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fluid-taylor",
    "outputId": "19584d54-5357-49ae-bc06-96bf66434867",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
        "| Arguments Remove cwd: ./Pendulum-v1_DDPG_6 \n",
        "|Step 4.00e+02  ExpR    -3.60  |ObjC     3.22  ObjA     0.15 \n",
        "|Step 4.00e+04  ExpR    -1.98  |ObjC     0.86  ObjA   -82.97 \n",
        "|Step 5.88e+04  ExpR    -1.58  |ObjC     0.82  ObjA   -66.20 \n",
        "|Step 7.32e+04  ExpR    -0.60  |ObjC     0.62  ObjA   -45.66 \n",
        "|Step 8.52e+04  ExpR    -0.47  |ObjC     0.36  ObjA   -33.60 \n",
        "|Step 9.56e+04  ExpR    -0.33  |ObjC     0.35  ObjA   -28.88 \n",
        "| UsedTime: 357 | SavedDir: ./Pendulum-v1_DDPG_6 \n",
        "\n",
        "| Arguments Keep cwd: ./Pendulum-v1_DDPG_6 \n",
        "|Steps          400  |Returns avg -1391.019  std   272.423 \n",
        "|Steps        40000  |Returns avg  -822.530  std    77.746 \n",
        "|Steps        58800  |Returns avg  -583.974  std    54.622 \n",
        "|Steps        73200  |Returns avg  -199.278  std    83.178 \n",
        "|Steps        85200  |Returns avg  -163.388  std    82.727 \n",
        "|Steps        95600  |Returns avg  -211.675  std    72.861 \n"
     ]
    }
   ],
   "source": [
    "from elegantrl_helloworld.agent import AgentPPO\n",
    "agent_class = AgentPPO\n",
    "env_name = [\"Pendulum-v1\", \"LunarLanderContinuous-v2\", \"BipedalWalker-v3\"][0]\n",
    "gpu_id = 0\n",
    "\n",
    "if env_name == \"Pendulum-v1\":\n",
    "    env = PendulumEnv()\n",
    "    env_func = PendulumEnv\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "    args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "    '''reward shaping'''\n",
    "    args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
    "    args.gamma = 0.97\n",
    "\n",
    "    '''network update'''\n",
    "    args.target_step = args.max_step * 8\n",
    "    args.net_dim = 2 ** 7\n",
    "    args.batch_size = 2 ** 8\n",
    "    args.repeat_times = 2 ** 4\n",
    "\n",
    "    '''evaluate'''\n",
    "    args.eval_gap = 2 ** 6\n",
    "    args.eval_times = 2 ** 3\n",
    "    args.break_step = int(8e5)\n",
    "elif env_name == \"LunarLanderContinuous-v2\":\n",
    "    import gym\n",
    "    env = gym.make(env_name)\n",
    "    env_func = gym.make\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "    args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "    '''reward shaping'''\n",
    "    args.reward_scale = 2 ** -2\n",
    "    args.gamma = 0.99\n",
    "\n",
    "    '''network update'''\n",
    "    args.target_step = args.max_step * 2\n",
    "    args.net_dim = 2 ** 7\n",
    "    args.batch_size = 2 ** 8\n",
    "    args.repeat_times = 2 ** 5\n",
    "\n",
    "    '''evaluate'''\n",
    "    args.eval_gap = 2 ** 6\n",
    "    args.eval_times = 2 ** 5\n",
    "    args.break_step = int(6e5)\n",
    "elif env_name == \"BipedalWalker-v3\":\n",
    "    import gym\n",
    "    env = gym.make(env_name)\n",
    "    env_func = gym.make\n",
    "    env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "    args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "    '''reward shaping'''\n",
    "    args.reward_scale = 2 ** -1\n",
    "    args.gamma = 0.98\n",
    "\n",
    "    '''network update'''\n",
    "    args.target_step = args.max_step\n",
    "    args.net_dim = 2 ** 8\n",
    "    args.batch_size = 2 ** 9\n",
    "    args.repeat_times = 2 ** 4\n",
    "\n",
    "    '''evaluate'''\n",
    "    args.eval_gap = 2 ** 6\n",
    "    args.eval_times = 2 ** 4\n",
    "    args.break_step = int(6e5)\n",
    "else:\n",
    "    raise ValueError(\"env_name:\", env_name)\n",
    "    \n",
    "args.learner_gpus = gpu_id\n",
    "train_agent(args)\n",
    "evaluate_agent(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce11979d",
   "metadata": {
    "id": "ce11979d"
   },
   "source": [
    "### Authors\n",
    "github [ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Demo_China_A_share_market.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "d9161d898770fc09463e4d697d40f3619206d01233bb8ab45ec62b646b625d48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
